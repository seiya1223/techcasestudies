---
layout: post
title:  "How Hark used cloud native services and DevOps to accelerate time to market"
author: "Marcus Robinson, Ross Smith"
author-link: "#"
#author-image: "{{ site.baseurl }}/images/authors/photo.jpg"
date:   2016-10-19
categories: DevOps
color: "blue"
#image: "{{ site.baseurl }}/images/imagename.png" #should be ~350px tall
excerpt: Hark is an IoT startup that needed an automated build and release pipeline to accelerate delivery of its IoT monitoring solution. Microsoft joined Hark to design the required pipelines and to implement them in VSTS.
verticals: [Pharmaceutics, Technology]
---

Hark is an IoT startup that required an automated build and release pipeline to accelerate the delivery of its IoT monitoring solution. Microsoft joined Hark to design the required build and release pipelines and to implement the pipeline in VSTS.

DevOps practices implemented:

- Infrastructure as Code
- Continuous Integration
- Continuous Delivery
- Automated Testing
- Application Monitoring

### Participants 
The core hack team included: 

- Jordan Appleson, CEO, Hark, @jordanisonfire
- Andrew Hathaway, CTO, Hark, @andrewhathaway
- Julian Kay, CSA, Hark, @juliankay
- Marcus Robinson, Technical Evangelist, Microsoft @techdiction
- Ross Smith, Technical Evangelist, Microsoft @ross_p_smith
- David Little, Premier Field Engineer, Microsoft

## Customer Profile ##
Hark is a technology company, based in the United Kingdom, building an interconnected cloud-based sensor platform that allows users to monitor, store and gain insight into their environmental data in real-time. Alongside their own hardware, they allow any industry standard sensor to connect to their platform with an almost plug and play nature for rapid deployments.

Some of the Hark team has a background in building enterprise software with Microsoft technologies. However, a great deal of the team's experience is with technologies such as Golang, MongoDB, Apache Cassandra, Apache Kafka, TypeScript and ReactJS. To accelerate the development process and allow the team to focus on the product rather than maintaining server-based services, they made the decision to use Azure native services wherever possible.

## Problem Statement ##

Hark requires a robust continuous delivery pipeline which will reduce operational overhead, ensure minimal downtime, and reduce the time taken for code to move from development to production. Hark is aiming to have its MVP in production use in October and a robust delivery pipeline is key to achieving this.

## Current Situation ##

Product is currently in "alpha" stages of development with a working prototype that is demonstrated to customers. The delivery pipeline currently exists of developers committing to GitHub and then carrying out manual deployments to a test environment. Hark is a small team with limited time and is looking to automate as much of the software delivery process as possible.

## Architecture Overview ##

The current architecture consists of a number of Azure services. This includes Azure WebApps that host both customer-facing websites and APIs, with a data layer consisting of Table Storage and SQL Azure databases. The architecture was sketched out, then Visio was used for clarification:

<img src="{{ site.baseurl }}/images/hark/architecture.jpg" width="200">


<img src="{{ site.baseurl }}/images/hark/hack_arch.png" width="700">


### Architecture Components
- An SPA accessed by users. This uses Node.js and is hosted within a virtual Machine on Digital Ocean. 
- Two applications providing APIs are currently hosted within a single Azure WebApp, although once the code base is separated will be hosted separately.
- A console application running inside an Azure virtual machine that processes data.
- Two EventHubs and an Azure Stream Analytics job that perform data calculations.
- A number of storage locations, including Azure Storage accounts and SQL Azure


## Designing Build and Release Pipelines ##
The first step in such an engagement would usually be to carry out a value stream mapping exercise to identify areas for process improvement. As Hark are an early stage startup not yet in production the software build and release process was ad-hoc and not defined. It soon became evident that trying to map nonexisting processes was not going to be a productive use of time.

As an alternative we began to discuss and map out build and release pipelines for each part of the application architecture.

<img src="{{ site.baseurl }}/images/hark/room_planning.jpg" />

<img src="{{ site.baseurl }}/images/hark/jordan_planning.jpg" />

<img src="{{ site.baseurl }}/images/hark/andrew_planning.jpg" />


### Front End

<img src="{{ site.baseurl }}/images/hark/tophat_build_test_stage.jpg" width="220">


<img src="{{ site.baseurl }}/images/hark/tophat_stage_prod.jpg" width="220">


### SenseNet

<img src="{{ site.baseurl }}/images/hark/sensenet_build.jpg" width="220">


<img src="{{ site.baseurl }}/images/hark/sensenet_test.jpg" width="220">


<img src="{{ site.baseurl }}/images/hark/sensenet_staging_production.jpg" width="220">


### Database

<img src="{{ site.baseurl }}/images/hark/sensenet_db_test.jpg" width="220">


<img src="{{ site.baseurl }}/images/hark/sensenet_db_production.jpg" width="220">


### Monitoring
Outside of the build and release pipeline it is important to consider monitoring. We spent a short time discussing how the application would be monitored once deployed.

<img src="{{ site.baseurl }}/images/hark/monitoring.jpg" width="220">


At the end of the day we discussed what areas Hark felt would be most beneficial to be the focus of a hack.

<img src="{{ site.baseurl }}/images/hark/need_help_with.jpg" width="220">


We decided to arrange a hack to focus on the following  items:

- Creating infrastructure using Azure Resource Manager templates
- Enabling automated testing of the APIs
- Creating a build and release pipeline that implements Continuous Integration and Continuous Delivery (CI/CD)
- Integrating Application Insights alerts in to Slack

------------------------------------

## The Hack


![Hack group]( {{ site.baseurl }}/images/hark_hack_group.jpg)

--------------------------------------

### Azure Resource Manager Templates
Azure Resource Manager templates allow solution infrastructure to be defined as code. The ensures consistency each time the application is deployed.

**Mapping ARM Template to Application Architecture** 

Since the hack the application architecture had been modified and clarified. The Payload API and Website API  have  been split and the processes running as a console application will be migrated from a virtual machine to Azure AppService Web Jobs. Using a Azure AppService Web Job as opposed to a VM will reduce administration overhead as there is no server overhead and facilitate autoscaling.

<img src="{{ site.baseurl }}/images/hark/hack_arch_new.png" width="600">


By reviewing the build and release pipelines created earlier and taking into consideration the architecture changes the decision was made to build separate ARM Templates for the following aspects of infrastructure:

- SenseNet
- TopHat
- Data Layer

**Creating the ARM Templates**

As Hark had already deployed the application using the Azure Portal rather than creating the templates from scratch they were exported using the "Automation script" feature of a Resource Group:

![Automation script]( {{ site.baseurl }}/images/hark_automation_script.png)

Once the template was exported a number of errors were presented:

![Automation script error bar]( {{ site.baseurl }}/images/hark_automation_script_error_bar.png)

![Automation script errors]( {{ site.baseurl }}/images/hark_automation_script_errors.png)

These errors are due to the fact that a number of resources are currently not supported for export, this included:

- Azure Stream Analytics
- Web App Config, such as application settings
- Web Site Extensions

There were a number of other issues with the template that would need to be resolved, including:

- The application had been deployed into a single resource group. We required multiple resource groups.
- The template had a large number of parameters that will never be used.
-  Selected resource properties that were exported show resource status/current configuration and cannot be set using an ARM template.

**Data Processing Template**

This template contains:

- WebApp with two web jobs
- Two Event Hubs
- Azure Stream Analytics
- Storage account used by the event processing web jobs

**Azure Stream Analytics**

Unfortunately there is no JSON schema available as of yet that defines how to create an ASA resource using ARM. However the REST API for ASA has been documented on MSDN - https://msdn.microsoft.com/en-us/library/azure/dn834994.aspx. Using the REST API, and some examples found online as part of the IoT suite (https://github.com/Azure/azure-iot-remote-monitoring/tree/master/Common/Deployment) and a fair amount of trial and error the ASA resource was successfully created,

```
{
      "apiVersion": "2015-06-01",
      "type": "Microsoft.StreamAnalytics/streamingjobs",
      "name": "[variables('stream_analytics_job_name')]",
      "location": "[resourceGroup().location]",
      "dependsOn": [
       ],
      "properties": {
        "sku": {
          "name": "standard"
        },
        "outputStartMode": "JobStartTime",
        "outputStartTime": null,
        "EventsOutOfOrderMaxDelayInSeconds": 10,
        "EventsOutOfOrderPolicy": "adjust",
        "Inputs": [
          {
            "Name": "[variables('sa_input_name')]",
            "Properties": {
              "DataSource": {
                "Properties": {
                  "Name": "[variables('sa_input_name')]",
                  "EventHubName": "[variables('eventhubs_name1')]",
                  "consumerGroupName": "[variables('consumergroups_name1')]",
                  "ServiceBusNamespace": "[variables('namespace_name')]",
                  "SharedAccessPolicyKey": "[listkeys(variables('namespace_auth_rule_id'), variables('namespace_api_version')).primaryKey]",
                  "SharedAccessPolicyName": "[variables('auth_rule_name')]",
                  "PartitionKey": "PartitionId"
                },
                "Type": "Microsoft.ServiceBus/EventHub"
              },
              "Serialization": {
                "Properties": {
                  "Encoding": "UTF8",
                  "Format": "Array"
                },
                "Type": "Json"
              },
              "Type": "Stream"
            }
          }
        ],
        "Outputs": [
          {
            "Name": "ConvertedMetricAverage",
            "Properties": {
              "DataSource": {
                "Properties": {
                  "EventHubName": "[variables('eventhubs_name2')]",
                  "consumerGroupName": "[variables('consumergroups_name2')]",

                  "ServiceBusNamespace": "[variables('namespace_name')]",
                  "SharedAccessPolicyKey": "[listkeys(variables('namespace_auth_rule_id'), variables('namespace_api_version')).primaryKey]",
                  "SharedAccessPolicyName": "[variables('auth_rule_name')]",
                  "PartitionKey": "PartitionId"
                },
                "Type": "Microsoft.ServiceBus/EventHub"
              },
              "Serialization": {
                "Properties": {
                  "Encoding": "UTF8",
                  "Format": "Array"
                },
                "Type": "Json"
              }
            }
          }
        ],
        "Transformation": {
          "Name": "[variables('sa_transformation_name')]",
          "Properties": {
            "Query": "SELECT\r\n  System.Timestamp as TimeValue,\r\n  SensorId,\r\n  AVG(Value) as ValueAvg,\r\n  COUNT(*) as ValueAvgCount,\r\n  SUM(Value) as ValueAvgSum,\r\n  MIN(Value) as ValueMin,\r\n  MAX(Value)  ValueMax,\r\n  '5MINUTE' as 'AverageType'\r\nINTO\r\n    ConvertedMetricAverages\r\nFROM\r\n    ConvertedMetrics1\r\nTIMESTAMP BY CreatedUtc\r\nGROUP BY SensorId, TUMBLINGWINDOW(minute, 5)\r\n",
            "StreamingUnits": 1
          }
        }
      }
    }
```

**WebApp Configuration**

Configuration for both WebApps needed to be provided within the ARM template. Some configuration items will change in each environment, and others will remain consistent. One example being Azure Application Insights. There is an instance of Application Insights for each WebApp in each environment.

The following JSON extract shows how the ```APPINSIGHTS_INSTRUMENTATIONKEY``` application setting is set based on an ```APPINSIGHTS_INSTRUMENTATIONKEY``` parameter passed in when deployed:

```
 "parameters": {
      "APPINSIGHTS_INSTRUMENTATIONKEY": {
      "type": "string"
    },
	...
 }

```
Setting the value in the appsettings resource:

```  
{
            "apiVersion": "2015-08-01",
            "name": "appsettings",
            "type": "config",
            "dependsOn": [
              "[resourceId('Microsoft.Web/Sites', variables('TopHatSiteName'))]"
            ],
            "properties": {
                "APPINSIGHTS_INSTRUMENTATIONKEY": "[parameters('APPINSIGHTS_INSTRUMENTATIONKEY')]",
         ...
             }
},
```


**Regions & Release Environment**

As some Azure resources, such as storage accounts, require a unique name across the entirety of Azure. To ensure redundancy across the application and hence ARM templates will be deployed in multiple regions. This means that each time the template is deployed parameters must be included to ensure uniqueness. We decided to pass in a parameter called EnvironmentSuffix such as dev, test, production; and a region suffix which will be set according to the region being deployed into, such as ne for North Europe, we for West Europe etc. Unique variables could have been generated but using a naming convention such as this ensures resources can be easily identified. 

```
 "parameters": {

    "EnvironmentSuffix": {
      "type": "string"
    },

    "RegionSuffix": {
      "type": "string"
    }
    ...
 }

```
```
  "EventHubStorageAccountName": "[concat('myappeh',parameters('EnvironmentSuffix'), parameters('RegionSuffix'))]",

```

The compete ARM template can be found here: [https://github.com/marrobi/ARMTemplates/tree/master/HARK](https://github.com/marrobi/ARMTemplates/tree/master/HARK) Please note that this was created during a hack, has been sanitized of certain information to enable publication and will need to be worked on further before reuse.

**Data Layer**

The template for the data layer proved to be much more straight forward. Once the exported template was cleaned up not a lot of work was required other than introducing the Environment and Region suffixes as above. We also ensure the required firewall rules were applied to the SQL server to allow remote management:
 ```
       {
          "type": "firewallrules",
          "kind": "v12.0",
          "name": "ClientIPAddress1",
          "apiVersion": "2014-04-01-preview",
          "location": "[resourceGroup().location]",
          "properties": {
            "startIpAddress": "123.123.123.123",
            "endIpAddress": "123.123.123.123"
          },
          "resources": [ ],
          "dependsOn": [
            "[resourceId('Microsoft.Sql/servers', variables('sqlserver_name'))]"
          ]
        },
```

The completed ARM template can be found here: [https://github.com/marrobi/ARMTemplates/tree/master/HARK](https://github.com/marrobi/ARMTemplates/tree/master/HARK). Again please note that this was created during a hack, has been sanitized of certain information to enable publication and will need to be worked on further before reuse.

**Traffic Manager**

As the application will be deployed across regions Traffic Manager is required to distribute requests. This was created separately to the other ARM templates as it is only required once per environment.


-------------------------------

### Pipeline Design
Initial implementation of Continuous Integration was aimed at getting the sln file to compile with a standard "Visual Studio Build" task. The solution was a ASP.Net Core WebAPI project that utilized .Net Framework 4.5.2 libraries.

**.NET Core Build Issues**

<img src="{{ site.baseurl }}/images/hark/dotnet_build_problems.jpg" width="400">


**Needed dotnet restore**

![Lock file error]( {{ site.baseurl }}/images/hark_lock_file_error.PNG)

The build warning informed us that we should run ```dotnet restore```, so we added a standard command task to call restore before the build task.

**Restore not working**

```
2016-09-08T08:07:42.5478829Z ##[section]Starting: Run dotnet
2016-09-08T08:07:42.6418840Z ##[command]dotnet restore
2016-09-08T08:07:44.2200223Z 
2016-09-08T08:07:44.2220204Z Welcome to .NET Core!
2016-09-08T08:07:44.2220204Z ---------------------
2016-09-08T08:07:44.2220204Z Learn more about .NET Core @ https://aka.ms/dotnet-docs. Use dotnet --help to see available commands or go to https://aka.ms/dotnet-cli-docs.
2016-09-08T08:07:44.2220204Z Telemetry
2016-09-08T08:07:44.2220204Z --------------
2016-09-08T08:07:44.2220204Z The .NET Core tools collect usage data in order to improve your experience. The data is anonymous and does not include commandline arguments. The data is collected by Microsoft and shared with the community.
2016-09-08T08:07:44.2220204Z You can opt out of telemetry by setting a DOTNET_CLI_TELEMETRY_OPTOUT environment variable to 1 using your favorite shell.
2016-09-08T08:07:44.2220204Z You can read more about .NET Core tools telemetry @ https://aka.ms/dotnet-cli-telemetry.
2016-09-08T08:07:44.2220204Z Configuring...
2016-09-08T08:07:44.2220204Z -------------------
2016-09-08T08:07:44.2220204Z A command is running to initially populate your local package cache, to improve restore speed and enable offline access. This command will take up to a minute to complete and will only happen once.
2016-09-08T08:07:49.0924123Z Decompressing 0%Decompressing 1%Decompressing 2%Decompressing 3%Decompressing 4%Decompressing 5%Decompressing 99%Expanding 100% 20920 ms
2016-09-08T08:08:33.2825951Z log  : Restoring packages for C:\a\1\s\src\SenseNet.Api\project.json...
2016-09-08T08:08:41.4638227Z error: Unable to resolve 'SenseNet.Azure' for '.NETFramework,Version=v4.5.2'.
2016-09-08T08:08:41.4648230Z error: Unable to resolve 'SenseNet.Common' for '.NETFramework,Version=v4.5.2'.
2016-09-08T08:08:41.4648230Z error: Unable to resolve 'SenseNet.Messaging.Azure' for '.NETFramework,Version=v4.5.2'.
2016-09-08T08:08:41.4648230Z error: Unable to resolve 'SenseNet.Services' for '.NETFramework,Version=v4.5.2'.
2016-09-08T08:08:41.4648230Z error: Unable to resolve 'SenseNet.Storage' for '.NETFramework,Version=v4.5.2'.
2016-09-08T08:08:41.4648230Z error: Unable to resolve 'SenseNet.Storage.Azure' for '.NETFramework,Version=v4.5.2'.
2016-09-08T08:08:41.4648230Z error: Unable to resolve 'SenseNet.Storage.SqlServer' for '.NETFramework,Version=v4.5.2'.

```

As you can see, there is a problem that ```dotnet restore``` does not work with mixed core/framework projects. See [https://github.com/dotnet/cli/issues/3199](https://github.com/dotnet/cli/issues/3199) and [https://github.com/aspnet/Tooling/issues/741](https://github.com/aspnet/Tooling/issues/741). We sought assistance and Jelle Druyts a Microsoft Premier Field Engineer pointed out that the dotnet executable is actually a wrapper around nuget. He suggested we try the latest version of Nuget directly from the Nuget website instead of ```dotnet restore```. We could have downloaded it and checked in the exe, but as it's a temporary fix, we download it on the fly; in this case the RC1 version. e.g. [https://dist.nuget.org/win-x86-commandline/v3.5.0-rc1/NuGet.exe](https://dist.nuget.org/win-x86-commandline/v3.5.0-rc1/NuGet.exe)

<img src="{{ site.baseurl }}/images/hark/NuGetDownload.png" width="600">


Then you can use the standard Nuget restore task but pointed explicitly at the NuGet.exe file that was just downloaded. To do this, set the "Advanced" "Path to NuGet.exe variable" to: ```$(build.stagingdirectory)\NuGet.exe```
    
After we had our solution building correctly, we needed to make sure that we copied all of the required outputs of the various projects to the $(Build.ArtifactsStagingDirectory) folder so we could publish them easily. Unfortunately, because this was .NET Core, it was not straightforward. Here are the steps that we put in place to enable this:

- We had to tell dotnet that we are publishing  to iis:

![Publish API]( {{ site.baseurl }}/images/hark_PublishAPI.PNG)

- We had to create a fake ```parameters.xml``` file and copy it to our ArtifactStagingDirectory so that we can use the 'Azure RM Web App Deployment' Task which requires it. It doesn't matter what is in there, but in our case, we had:

```
<parameters> <parameter name='IIS Web Application Name' defaultValue='MyApp' tags='IisApp' /> </parameters>
```

- We had to zip our .NET Core application up as it does not create a web deploy package automatically. A key thing to note is to ensure that the  'Prefix root folder name to archive paths' is unchecked as we do not want to preserve the file paths in the zip:

<img src="{{ site.baseurl }}/images/hark/ZipPackage.PNG" width="700">


The build definition with all the required steps was as follow: 

![Build definition]( {{ site.baseurl }}/images/hark_BuildDefinition.PNG)

### Release
We had further discussions over method for deployment of resource groups at each stage of pipeline. 

![Hack release]( {{ site.baseurl }}/images/hark_hack_release.jpg)

As the application is still in early stages of deployment it was decided to just have two environments, Development and Production environment. Hark intend to add a separate Testing environment at a later point in time. The following image shows the release pipeline for the deployment of the ARM Templates and associated WebApps.

<img src="{{ site.baseurl }}/images/hark/ReleasePipeline.PNG" width="700">


### Testing
The platform includes two APIs, one that receives data from the IoT gateway - the Payload API, and a second that is accessed by the web front end. A solution for testing these APIs was required that met the following criteria:

- Allowed calls to be made to the payload API and the result to be verified by making a call to the web API
- Could be used to run tests within a release pipeline in VSTS, i.e. supports command line execution of tests

After identifying a number of tools that could potentially help Postman ([https://www.getpostman.com](https://www.getpostman.com)) was selected as it is actively maintained and test collections can be executed at the command line using an OSS command line test runner for Postman called Newman [https://github.com/postmanlabs/newman](https://github.com/postmanlabs/newman).

<img src="{{ site.baseurl }}/images/hark/postman_write_test.png" width="700">


The test took the form of calls to the API and then verification that the correct responses were returned. These tests were saved as a collection and could be run within the UI and also exported as a json file. 

<img src="{{ site.baseurl }}/images/hark/postman_collection.png" width="300">


The tool also allows environmental variables to be pulled out into a ```postman_environment.json``` file so that we can use the same tests with different URI's which is perfect for our Release Definition.

<img src="{{ site.baseurl }}/images/hark/postman_files.png" width="400">


Once we were happy our Postman Collection, we checked in the json files and ensured that they were published as an artefact of the build so that we could us to use them in the release pipeline. 

To use the Postman tests in the release pipeline we used Newman. We added an ```npm install newman``` task to the pipeline and from a command line task called newman to execute the tests. We  also redirected the output to be a jUnit test result:

<img src="{{ site.baseurl }}/images/hark/NewmanTask.PNG" width="700">


This enabled us to the 'Publish test results' task to get the results displayed as part of the Release.

###  Application Insights
When it came to integrating Application Insights alerts into Slack we soon discovered that Hark had carried this out themselves prior to the hack! For reference they had used the Azure quickstart template from: [https://github.com/Azure/azure-quickstart-templates/tree/master/201-alert-to-slack-with-logic-app](https://github.com/Azure/azure-quickstart-templates/tree/master/201-alert-to-slack-with-logic-app).

Further work will need to be done to integrate this into the new instances of Application Insights in the release pipeline.

## Learnings
Other than the many technical learnings documented here, we came away with a couple more general insights:

- There are often multiple ways to reach a desired goal, especially when designing release pipelines.
- Value Stream Mapping is an extremely powerful way of identifying and removing areas of waste in the software development process.

However, as Hark are a startup with few defined processes, it became apparent that it was not appropriate to carry out a VSM. This is something we should remember when engaging with organizations with products at a similar stage of development. It would be interesting to carry out a VSM with Hark in 6 or 12 months time once processes have matured.

## Conclusion
Despite a number of technical hurdles needing to be overcome, we made great progress during the hackfest. It has also been great to learn that since the hackfest Hark have continued to add to the work carried out to create a comprehensive build and release pipeline. The following quotation illustrates what was achieved:

 >  During the hack we deployed and tested a robust CI pipeline with specific Azure native services. This included services that we were unsure about how to continuously deploy, such as Stream Analytics and EventHubs. Moving all our systems into VSTS (including our source control away from GitHub) was one of our main achievements along with deployment of geo-redundant deployment of application and infrastructure. 
 >
 >The hack greatly accelerated the deployment of these pipelines and we learnt a vast amount about the Microsoft tooling surrounding VSTS and Azure. The work will enable us to be in a production several times faster than our predicted roadmap and enable us to bring our first customers online in the coming weeks.
 >
 > <cite>Jordan Appleson, CEO, Hark</cite>
 

## Resources 
- ARM Templates [https://github.com/marrobi/ARMTemplates](https://github.com/marrobi/ARMTemplates)
- .NET Core [https://www.microsoft.com/net/core/platform](https://www.microsoft.com/net/core/platform)
- Postman [https://www.getpostman.com](https://www.getpostman.com)
- Newman [https://github.com/postmanlabs/newman](https://github.com/postmanlabs/newman)
